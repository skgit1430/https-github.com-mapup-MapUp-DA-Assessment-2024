# -*- coding: utf-8 -*-
"""python_section_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/111tyWeasv2EK1489G5LJWsVD30iclURK
"""

from typing import Dict, List

import pandas as pd

def reverse_by_n_elements(lst: List[int], n: int) -> List[int]:
    """
    Reverses the input list by groups of n elements.
    """
    result = []
    length = len(lst)


    for i in range(0, length, n):

        temp = []
        for j in range(i, min(i + n, length)):
            temp.append(lst[j])
        # Reverse the temp list manually
        for k in range(len(temp) - 1, -1, -1):
            result.append(temp[k])

    return result

from typing import Dict, List

def group_by_length(lst: List[str]) -> Dict[int, List[str]]:
    """
    Groups the strings by their length and returns a dictionary.
    """
    result = {}
    for word in lst:
        length = len(word)
        if length not in result:
            result[length] = []
        result[length].append(word)


    sorted_result = dict(sorted(result.items()))

    return sorted_result

def flatten_dict(nested_dict: Dict, sep: str = '.') -> Dict:
    """
    Flattens a nested dictionary into a single-level dictionary with dot notation for keys.

    :param nested_dict: The dictionary object to flatten
    :param sep: The separator to use between parent and child keys (defaults to '.')
    :return: A flattened dictionary
    """
    flat_dict = {}

    def _flatten(current_dict, parent_key=''):
        for key, value in current_dict.items():
            new_key = parent_key + sep + key if parent_key else key
            if isinstance(value, dict):
                _flatten(value, new_key)
            else:
                flat_dict[new_key] = value

    _flatten(nested_dict)
    return flat_dict

from typing import Dict

def flatten_dict(nested_dict: Dict, sep: str = '.') -> Dict:
    """
    Flattens a nested dictionary into a single-level dictionary with dot notation for keys.

    :param nested_dict: The dictionary object to flatten
    :param sep: The separator to use between parent and child keys (defaults to '.')
    :return: A flattened dictionary
    """
    flat_dict = {}

    def _flatten(current_dict, parent_key=''):
        if isinstance(current_dict, dict):
            for key, value in current_dict.items():
                new_key = parent_key + sep + key if parent_key else key
                _flatten(value, new_key)
        elif isinstance(current_dict, list):
            for index, item in enumerate(current_dict):
                new_key = f"{parent_key}[{index}]"
                _flatten(item, new_key)
        else:
            flat_dict[parent_key] = current_dict

    _flatten(nested_dict)
    return flat_dict

# Example input
nested_example = {
    "road": {
        "name": "Highway 1",
        "length": 350,
        "sections": [
            {
                "id": 1,
                "condition": {
                    "pavement": "good",
                    "traffic": "moderate"
                }
            }
        ]
    }
}

# Test the function
print(flatten_dict(nested_example))

from typing import List

def unique_permutations(nums: List[int]) -> List[List[int]]:
    """
    Generate all unique permutations of a list that may contain duplicates.

    :param nums: List of integers (may contain duplicates)
    :return: List of unique permutations
    """
    def backtrack(start, end):
        if start == end:
            permutations.append(nums[:])  # Add a copy of nums to the permutations list
        for i in range(start, end):
            if i != start and nums[i] == nums[start]:
                continue  # Skip duplicates
            nums[start], nums[i] = nums[i], nums[start]  # Swap to place current element at the start
            backtrack(start + 1, end)
            nums[start], nums[i] = nums[i], nums[start]  # Swap back (backtrack)

    nums.sort()  # Sorting helps to handle duplicates by skipping them
    permutations = []
    backtrack(0, len(nums))
    return permutations

# Example test
print(unique_permutations([1, 1, 2]))  # Output: [[1, 1, 2], [1, 2, 1], [2, 1, 1]]

import re
from typing import List

def find_all_dates(text: str) -> List[str]:
    """
    This function takes a string as input and returns a list of valid dates
    in 'dd-mm-yyyy', 'mm/dd/yyyy', or 'yyyy.mm.dd' format found in the string.

    Parameters:
    text (str): A string containing the dates in various formats.

    Returns:
    List[str]: A list of valid dates in the formats specified.
    """
    # Regular expression to match the three date formats
    date_pattern = r'\b\d{2}-\d{2}-\d{4}\b|\b\d{2}/\d{2}/\d{4}\b|\b\d{4}\.\d{2}\.\d{2}\b'

    # Find all matches in the text
    dates = re.findall(date_pattern, text)

    return dates

# Example input
text = "I was born on 23-08-1994, my friend on 08/23/1994, and another one on 1994.08.23."
# Test the function
print(find_all_dates(text))  # Output: ["23-08-1994", "08/23/1994", "1994.08.23"]

!pip install polyline # Installs the necessary 'polyline' library

import pandas as pd
import polyline # Now this import should work
import numpy as np

def haversine(lat1, lon1, lat2, lon2):
    """
    Calculate the Haversine distance between two points on the Earth specified in decimal degrees.

    Args:
        lat1 (float): Latitude of the first point.
        lon1 (float): Longitude of the first point.
        lat2 (float): Latitude of the second point.
        lon2 (float): Longitude of the second point.

    Returns:
        float: Distance between the two points in meters.
    """
    # Convert latitude and longitude from degrees to radians
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])

    # Haversine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    r = 6371000  # Radius of Earth in meters
    return c * r

def polyline_to_dataframe(polyline_str: str) -> pd.DataFrame:
    """
    Converts a polyline string into a DataFrame with latitude, longitude, and distance between consecutive points.

    Args:
        polyline_str (str): The encoded polyline string.

    Returns:
        pd.DataFrame: A DataFrame containing latitude, longitude, and distance in meters.
    """
    # Decode the polyline string into a list of (latitude, longitude) tuples
    coordinates = polyline.decode(polyline_str)

    # Create a DataFrame from the coordinates
    df = pd.DataFrame(coordinates, columns=['latitude', 'longitude'])

    # Calculate distances
    distances = [0.0]  # First row has distance 0
    for i in range(1, len(coordinates)):
        distance = haversine(coordinates[i-1][0], coordinates[i-1][1],
                             coordinates[i][0], coordinates[i][1])
        distances.append(distance)

    df['distance'] = distances
    return df

# Example usage (uncomment to test):
# polyline_str = "your_encoded_polyline_here"
# df = polyline_to_dataframe(polyline_str)
# print(df)

from typing import List

def rotate_and_multiply_matrix(matrix: List[List[int]]) -> List[List[int]]:
    """
    Rotate the given matrix by 90 degrees clockwise, then transform each element
    by replacing it with the sum of all elements in the same row and column
    excluding itself.

    Args:
    - matrix (List[List[int]]): 2D list representing the matrix to be transformed.

    Returns:
    - List[List[int]]: A new 2D list representing the transformed matrix.
    """
    n = len(matrix)

    # Rotate the matrix 90 degrees clockwise
    rotated_matrix = [[0] * n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            rotated_matrix[j][n - 1 - i] = matrix[i][j]

    # Transform the rotated matrix
    final_matrix = [[0] * n for _ in range(n)]

    for i in range(n):
        for j in range(n):
            row_sum = sum(rotated_matrix[i])  # Sum of the entire row
            col_sum = sum(rotated_matrix[k][j] for k in range(n))  # Sum of the entire column
            final_matrix[i][j] = row_sum + col_sum - rotated_matrix[i][j]  # Exclude itself

    return final_matrix

# Example usage
matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
result = rotate_and_multiply_matrix(matrix)
print(result)  # Output: [[22, 19, 16], [23, 20, 17], [24, 21, 18]]

import pandas as pd
import os

def time_check(df: pd.DataFrame) -> pd.Series:
    """
    Use shared dataset-1 to verify the completeness of the data by checking whether the timestamps for each unique (`id`, `id_2`) pair cover a full 24-hour and 7 days period.

    Args:
        df (pandas.DataFrame): The DataFrame containing id, id_2, and timestamp columns.

    Returns:
        pd.Series: A boolean series indicating whether each (id, id_2) pair has incorrect timestamps.
    """

    # Ensure the DataFrame has the correct columns
    required_columns = ['id', 'id_2', 'startDay', 'startTime', 'endDay', 'endTime']
    if not all(col in df.columns for col in required_columns):
        raise ValueError("Input DataFrame must contain the columns: " + ", ".join(required_columns))

    # Combine startDay and startTime into a single datetime column
    df['start'] = pd.to_datetime(df['startDay'] + ' ' + df['startTime'])
    df['end'] = pd.to_datetime(df['endDay'] + ' ' + df['endTime'])

    # Create a MultiIndex based on id and id_2
    grouped = df.groupby(['id', 'id_2'])

    def check_time_coverage(group):
        # Check if there are records for all 7 days
        days_covered = group['start'].dt.dayofweek.nunique() == 7

        # Check if the time spans a full 24-hour period
        time_covered = group['start'].min().date() == group['end'].max().date() and \
                       group['end'].max() - group['start'].min() >= pd.Timedelta(days=1)

        return not (days_covered and time_covered)

    # Apply the check for each (id, id_2) group
    result = grouped.apply(check_time_coverage)

    # Convert the result to a boolean Series with a MultiIndex
```python
import pandas as pd
import os

def time_check(df: pd.DataFrame) -> pd.Series:
    """
    Use shared dataset-1 to verify the completeness of the data by checking whether the timestamps for each unique (`id`, `id_2`) pair cover a full 24-hour and 7 days period.

    Args:
        df (pandas.DataFrame): The DataFrame containing id, id_2, and timestamp columns.

    Returns:
        pd.Series: A boolean series indicating whether each (id, id_2) pair has incorrect timestamps.
    """

    # Ensure the DataFrame has the correct columns
    required_columns = ['id', 'id_2', 'startDay', 'startTime', 'endDay', 'endTime']
    if not all(col in df.columns for col in required_columns):
        raise ValueError("Input DataFrame must contain the columns: " + ", ".join(required_columns))

    # Combine startDay and startTime into a single datetime column
    df['start'] = pd.to_datetime(df['startDay'] + ' ' + df['startTime'])
    df['end'] = pd.to_datetime(df['endDay'] + ' ' + df['endTime'])

    # Create a MultiIndex based on id and id_2
    grouped = df.groupby(['id', 'id_2'])

    def check_time_coverage(group):
        # Check if there are records for all 7 days
        days_covered = group['start'].dt.dayofweek.nunique() == 7

        # Check if the time spans a full 24-hour period
        time_covered = group['start'].min().date() == group['end'].max().date() and \
                       group['end'].max() - group['start'].min() >= pd.Timedelta(days=1)

        return not (days_covered and time_covered)

    # Apply the check for each (id, id_2) group
    result = grouped.apply(check_time_coverage)

    # Convert the result to a boolean Series with a MultiIndex
    return result

# Example usage (uncomment to test):
# Get the current directory of the script
#current_dir = os.path.dirname(os.path.abspath(__file__))
# Construct the full path to the CSV file
#file_path = os.path.join(current_dir, 'C:\Users\Lenovo\Downloads\dataset-1.csv')
# Read the CSV file using the constructed path
#df = pd.read_csv('C:/Users/Lenovo/Downloads/dataset-1.csv') #Corrected the file path
#print(time_check(df))
```

def time_check(df: pd.DataFrame) -> pd.Series:
    """
    Use shared dataset-1 to verify the completeness of the data by checking whether the timestamps for each unique (`id`, `id_2`) pair cover a full 24-hour and 7 days period.

    Args:
        df (pandas.DataFrame): The DataFrame containing id, id_2, and timestamp columns.

    Returns:
        pd.Series: A boolean series indicating whether each (id, id_2) pair has incorrect timestamps.
    """

    # Ensure the DataFrame has the correct columns
    required_columns = ['id', 'id_2', 'startDay', 'startTime', 'endDay', 'endTime']
    if not all(col in df.columns for col in required_columns):
        raise ValueError("Input DataFrame must contain the columns: " + ", ".join(required_columns))

    # Combine startDay and startTime into a single datetime column
    df['start'] = pd.to_datetime(df['startDay'] + ' ' + df['startTime'])
    df['end'] = pd.to_datetime(df['endDay'] + ' ' + df['endTime'])

    # Create a MultiIndex based on id and id_2
    grouped = df.groupby(['id', 'id_2'])

    def check_time_coverage(group):
        # Check if there are records for all 7 days
        days_covered = group['start'].dt.dayofweek.nunique() == 7

        # Check if the time spans a full 24-hour period
        time_covered = group['start'].min().date() == group['end'].max().date() and \
                       group['end'].max() - group['start'].min() >= pd.Timedelta(days=1)

        return not (days_covered and time_covered)

    # Apply the check for each (id, id_2) group
    result = grouped.apply(check_time_coverage)

    # Convert the result to a boolean Series with a MultiIndex
    return result

# Example usage (uncomment to test):
# Get the current directory of the script
current_dir = os.path.dirname(os.path.abspath(__file__))
# Construct the full path to the CSV file
file_path = os.path.join(current_dir, 'C:\Users\Lenovo\Downloads\dataset-1.csv')
# Read the CSV file using the constructed path
df = pd.read_csv('C:/Users/Lenovo/Downloads/dataset-1.csv') #Corrected the file path
print(time_check(df))
```

def time_check(df: pd.DataFrame) -> pd.Series:
    """
    Use shared dataset-1 to verify the completeness of the data by checking whether the timestamps for each unique (`id`, `id_2`) pair cover a full 24-hour and 7 days period.

    Args:
        df (pandas.DataFrame): The DataFrame containing id, id_2, and timestamp columns.

    Returns:
        pd.Series: A boolean series indicating whether each (id, id_2) pair has incorrect timestamps.
    """

    # Ensure the DataFrame has the correct columns
    required_columns = ['id', 'id_2', 'startDay', 'startTime', 'endDay', 'endTime']
    if not all(col in df.columns for col in required_columns):
        raise ValueError("Input DataFrame must contain the columns: " + ", ".join(required_columns))

    # Combine startDay and startTime into a single datetime column
    df['start'] = pd.to_datetime(df['startDay'] + ' ' + df['startTime'])
    df['end'] = pd.to_datetime(df['endDay'] + ' ' + df['endTime'])

    # Create a MultiIndex based on id and id_2
    grouped = df.groupby(['id', 'id_2'])

    def check_time_coverage(group):
        # Check if there are records for all 7 days
        days_covered = group['start'].dt.dayofweek.nunique() == 7

        # Check if the time spans a full 24-hour period
        time_covered = group['start'].min().date() == group['end'].max().date() and \
                       group['end'].max() - group['start'].min() >= pd.Timedelta(days=1)

        return not (days_covered and time_covered)

    # Apply the check for each (id, id_2) group
    result = grouped.apply(check_time_coverage)

    # Convert the result to a boolean Series with a MultiIndex
    return result

# Example usage (uncomment to test):
# Get the current directory of the script
current_dir = os.path.dirname(os.path.abspath(__file__))
# Construct the full path to the CSV file
file_path = os.path.join(current_dir, 'C:\Users\Lenovo\Downloads\dataset-1.csv')
# Read the CSV file using the constructed path
df = pd.read_csv('C:/Users/Lenovo/Downloads/dataset-1.csv') #Corrected the file path
print(time_check(df))
```
print(time_check(df))